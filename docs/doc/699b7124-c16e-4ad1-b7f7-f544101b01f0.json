{
    "summary": "The code preprocesses text data, formats lines, and performs windowed convolution or extracts excerpts using a convolver. It then iterates through cleaned conversation groups, generating four processed line versions and yielding them if withOriginalLine is true, or only the processed lines otherwise.",
    "details": [
        {
            "comment": "This code defines several functions for preprocessing text data, including removing duplicate characters and stripping certain characters. The final function takes a string read from a file and returns line-wise cleaned and merged conversation groups with a line index mapping.",
            "location": "\"/media/root/Toshiba XG3/works/lazero/docs/src/lazero/search/preprocessing.py\":0-34",
            "content": "def removeDuplicates(line, chars=[\" \", \"\\t\"], maxConsecutiveLength=1):\n    for char in chars:\n        minUnallowedConsecutiveLength = maxConsecutiveLength + 1\n        while True:\n            if char * minUnallowedConsecutiveLength in line:\n                line = line.replace(\n                    char * minUnallowedConsecutiveLength, char * maxConsecutiveLength\n                )\n            else:\n                break\n    return line\ndef stripChars(line, chars=[\" \", \"\\t\"]):\n    while True:\n        flag = False\n        for char in chars:\n            if line.startswith(char) or line.endswith(char):\n                line = line.strip(char)\n                flag = True\n        if not flag:\n            break\n    return line\ndef standardLineCleaner(line):\n    line = removeDuplicates(line)\n    line = stripChars(line)\n    return line\ndef getLineWiseAndListOfCleanedMergedConvGroupWithLineIndexMappingFromStringReadFromFile(\n    data, char_per_group=30, group_per_conv_group=3, step_group_for_conv=2\n):\n    # data is the whole string read from file"
        },
        {
            "comment": "The code cleans the input data, splits it into lines, and creates a mapping between characters and their corresponding line numbers. It also checks for certain conditions like group sizes and applies formatting rules to create a new content with potential overlapping groups.",
            "location": "\"/media/root/Toshiba XG3/works/lazero/docs/src/lazero/search/preprocessing.py\":36-62",
            "content": "    data = data.replace(\"\\r\\n\", \"\\n\")\n    # the original data, shall be used as reference. save it somewhere, like database.\n    linewise = data.split(\"\\n\")  # there won't be \"\\n\" in the line.\n    # instead of 1. just to make sure these conv groups overlap.\n    assert step_group_for_conv >= 1\n    assert (\n        step_group_for_conv <= group_per_conv_group\n    )  # at least there is no gap, though when equal there will be no overlapping.\n    assert group_per_conv_group >= 1\n    assert char_per_group >= 1\n    # rule to add space: if there's \"-\" ending, remove the \"-\" then directly concat with another line.\n    # if not, then make sure there's one space between two lines.\n    # create char index to line index mapping.\n    newContent = \"\"\n    newContentCharIndexToLineIndexDict = {}\n    alphabets = \"abcdefghijklmnopqrstuvwxyz\"\n    alphabets += \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n    import string\n    english_punctuation = string.punctuation\n    for lineNumber, line in enumerate(linewise):\n        line_cleaned = standardLineCleaner(line)"
        },
        {
            "comment": "This code checks if a line is zero length, and if so, it skips without doing anything. If the line ends with a hyphen or has an alphabet character at the end, it modifies the line accordingly. It then updates a dictionary mapping content length to line index for each character in the modified line. Finally, it appends the modified line to newContent and updates the newContentCharIndexToLineIndexDict.",
            "location": "\"/media/root/Toshiba XG3/works/lazero/docs/src/lazero/search/preprocessing.py\":63-82",
            "content": "        # for zero length line (after cleaned), we skip without doing anything.\n        if len(line_cleaned) == 0:\n            continue\n        # print(\"{}:\".format(lineNumber), line_cleaned)\n        # this process will never decrease the length of the line.\n        # i guess the symbol is different somehow. the hyphen is not avaliable on keyboard.\n        if line_cleaned.endswith(\"-\") or line_cleaned.endswith(\"\u2010\"):\n            line_cleaned = line_cleaned[:-1]\n        elif line_cleaned[-1] in alphabets + english_punctuation:\n            line_cleaned += \" \"\n        # we shall get the length again, cause we have processed this thing.\n        lineCleanedLength = len(line_cleaned)\n        newContentLength = len(newContent)\n        mDict = {\n            newContentLength + index: lineNumber for index in range(lineCleanedLength)\n        }\n        newContent += line_cleaned\n        newContentCharIndexToLineIndexDict.update(\n            mDict\n        )  # this shall be the most memory intensive object. delete it after use."
        },
        {
            "comment": "This code performs windowed convolution or extracts excerpts from a given text using a convolver. It checks for the appropriate end index by comparing it with the newContent length and creates a list of cleaned merged conv groups with line index mapping.",
            "location": "\"/media/root/Toshiba XG3/works/lazero/docs/src/lazero/search/preprocessing.py\":84-104",
            "content": "    # now, how to do convolution, or the windowed conv-like excerpt creation?\n    # print(\"MAX KEY:\", max(list(newContentCharIndexToLineIndexDict.keys())))\n    # MAX KEY: 85783\n    # which is smaller than:\n    # KeyError: 85830\n    # so it is obvious that we need the smaller 'endIndex', by using min(endIndex, newContentLength)\n    # breakpoint()\n    newContentLength = len(newContent)\n    startIndex = 0\n    listOfCleanedMergedConvGroupWithLineIndexMapping = []\n    # maybe you want to merge the fetched 'cleanedMergedConvGroup' according to 'lineIndexMapping', but that's another story.\n    # you can use the mathlib, from pyjom.\n    # i think the mathlib should be embedded to lazero. pyjom's mathlib can be grabbed from there.\n    while True:\n        if startIndex >= newContentLength:  # does not break? wtf?\n            break\n        endIndexOffset = group_per_conv_group * char_per_group\n        endIndex = startIndex + endIndexOffset\n        endIndex = min(endIndex, newContentLength - 1)\n        # if endIndex <= startIndex:  # failsafe."
        },
        {
            "comment": "Appending cleaned merged conv groups with line index mapping to list, adjusting startIndex, removing dict.",
            "location": "\"/media/root/Toshiba XG3/works/lazero/docs/src/lazero/search/preprocessing.py\":105-129",
            "content": "        #     continue\n        # the append process.\n        lineIndexStart = newContentCharIndexToLineIndexDict[\n            startIndex\n        ]  # maybe not just one line?\n        lineIndexEnd = newContentCharIndexToLineIndexDict[endIndex]  # key error? wtf?\n        lineIndicesTuple = (lineIndexStart, lineIndexEnd)\n        mElem = {\n            \"conv_group_merged\": newContent[startIndex:endIndex],\n            \"line_range\": lineIndicesTuple,\n        }\n        listOfCleanedMergedConvGroupWithLineIndexMapping.append(\n            mElem\n        )  # this shall be the thing that we need. just maybe.\n        # add to startIndex.\n        startIndex += step_group_for_conv * char_per_group\n    del newContentCharIndexToLineIndexDict\n    return linewise, listOfCleanedMergedConvGroupWithLineIndexMapping\n# now we have to process the 'listOfCleanedMergedConvGroupWithLineIndexMapping' list, make each line into 4 corresponding processed line.\n# no original line! original line is noisy.\n# that is not original line. we need something other than that."
        },
        {
            "comment": "This function removes punctuation from a line of text, cleans the resulting string, and then uses Jieba to segment the cleaned text into individual words. The final words are stored in a list and returned.",
            "location": "\"/media/root/Toshiba XG3/works/lazero/docs/src/lazero/search/preprocessing.py\":131-160",
            "content": "# hint: we do not want to store all these shit in our tiny little memory. we want it 'IN DATABASE'\n# from lazero.search.postprocessing import porterStemmer\nfrom nltk.stem import PorterStemmer\nporterStemmer = PorterStemmer()  # whill automatically get lower case.\nimport wordninja\nimport string\nfrom zhon.hanzi import punctuation as chinese_punctuation\nimport jieba\ndef getWordsWithoutPunctuation(line):\n    chinese_and_english_punctuation = set(\n        list(string.punctuation + chinese_punctuation)\n    )\n    line = line.replace(\"\\n\", \" \")\n    for punctuation in chinese_and_english_punctuation:\n        line = line.replace(punctuation, \" \")\n    cleaned_line = standardLineCleaner(line)\n    # now use what?\n    # split with what first?\n    # wordninja. split words.\n    # nope. we use jieba first.\n    jieba_cutted_words = jieba.lcut(cleaned_line)  # remove whitespace!\n    final_words = []\n    for word in jieba_cutted_words:\n        strip_word = word.strip()\n        if len(strip_word) == 0:\n            # we should only keep the splited words."
        },
        {
            "comment": "1. Extract words from line without punctuation\n2. Split each word into cutted version\n3. If no cutted version, keep the original word\n4. Stem all original words using PorterStemmer\n5. Join all results (words, cutted versions, stemmed versions) with space for further processing",
            "location": "\"/media/root/Toshiba XG3/works/lazero/docs/src/lazero/search/preprocessing.py\":161-187",
            "content": "            continue\n        else:\n            final_words.append(word)\n    return final_words\ndef getFourVersionsOfProcessedLine(line, debug=False):\n    global porterStemmer\n    # from nltk.stem import PorterStemmer\n    stemmer = porterStemmer\n    final_words = getWordsWithoutPunctuation(line)\n    final_cutted_words = []\n    for word in final_words:\n        ninja_cutted_word = wordninja.split(word)\n        if len(ninja_cutted_word) == 0:\n            # we shall keep the original word.\n            final_cutted_words.append(word)\n        else:\n            final_cutted_words.extend(ninja_cutted_word)\n    # now 'stem' words use nltk.\n    final_stemmed_words = [stemmer.stem(word) for word in final_words]\n    final_cutted_stemmed_words = [stemmer.stem(word) for word in final_cutted_words]\n    # finally, join all things with space, for whatever reason.\n    final_line = \" \".join(final_words)  # for our dearly transformer\n    final_cutted_line = \" \".join(final_cutted_words)  # for our dearly whoosh\n    final_stemmed_line = \" \".join(final_stemmed_words)  # for our dearly whoosh"
        },
        {
            "comment": "The code is generating four different versions of a line by cutting, stemming, and combining the words. It then returns these versions as well as the original line (optional). This can be used to process both query and data for search engines but may increase index size significantly. The debug mode prints the final line, cutted line, stemmed line, and cutted-stemmed line.",
            "location": "\"/media/root/Toshiba XG3/works/lazero/docs/src/lazero/search/preprocessing.py\":188-214",
            "content": "    final_cutted_stemmed_line = \" \".join(\n        final_cutted_stemmed_words\n    )  # for our dearly whoosh\n    # how to use these four things?\n    # use them all for all search engines? that will increase our index size significantly!\n    # problem is, both query and data need to be processed somehow. but how?\n    # you want to use different split methods at the same time, or one at a time?\n    # you want to score them right? mostly in whoosh!\n    if debug:\n        from lazero.utils.logger import sprint\n        print(\"final_line\")\n        sprint(final_line)\n        print(\"final_cutted_line\")\n        sprint(final_cutted_line)\n        print(\"final_stemmed_line\")\n        sprint(final_stemmed_line)\n        print(\"final_cutted_stemmed_line\")\n        sprint(final_cutted_stemmed_line)\n    return final_line, final_cutted_line, final_stemmed_line, final_cutted_stemmed_line\ndef getFourVersionOfLineInListOfCleanedMergedConvGroupWithLineIndexMapping(\n    listOfCleanedMergedConvGroupWithLineIndexMapping, withOriginalLine=False"
        },
        {
            "comment": "This code is iterating through a list of cleaned and merged conversation groups. It calls the function \"getFourVersionsOfProcessedLine\" to generate four processed line versions (final_line, final_cutted_line, final_stemmed_line, final_cutted_stemmed_line) from each group. If withOriginalLine is true, it yields the original conversation group, along with the four processed lines. Otherwise, it only yields the four processed lines.",
            "location": "\"/media/root/Toshiba XG3/works/lazero/docs/src/lazero/search/preprocessing.py\":215-229",
            "content": "):\n    for elem in listOfCleanedMergedConvGroupWithLineIndexMapping:\n        conv_group_merged = elem[\"conv_group_merged\"]\n        # what you want to yield?\n        (\n            final_line,\n            final_cutted_line,\n            final_stemmed_line,\n            final_cutted_stemmed_line,\n        ) = getFourVersionsOfProcessedLine(conv_group_merged)\n        if withOriginalLine:\n            yield conv_group_merged, final_line, final_cutted_line, final_stemmed_line, final_cutted_stemmed_line\n        else:\n            yield final_line, final_cutted_line, final_stemmed_line, final_cutted_stemmed_line\n        # yield (getFourVersionsOfProcessedLine(conv_group_merged),line_range)"
        }
    ]
}